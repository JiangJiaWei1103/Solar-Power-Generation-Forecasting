# Common configuration for training and evaluation processes
# ==Training==
seed: 168
device: 'cuda:0'
epochs: 200
loss_fn:
    name: 'l2'

# ==DataLoader==
dataloader:
    batch_size: 64
    shuffle: True
    num_workers: 8

# ==Solver==
solver:
    # ==Optimizer==
    optimizer:
        name: adam
        # Learning rate
        lr: 0.1
        # Weight decay, default=1e-2 for AdamW
        weight_decay: 0
        # ==Ada* series==
        # Term added to the denominator to improve numerical stability
        eps: 1e-8
        # ==Adadelta==
        # Coefficient used for computing a running avg of squared gradients
        rho: null
        # ==Adagrad==
        # Learning rate decay
        lr_decay: null
        # ==Adam* series==
        # Coefficients used for computing running avg of grad and its square
        beta1: 0.9
        beta2: 0.999
        #  Whether to use the AMSGrad variant
        amsgrad: False
        # ==SDG==
        # Momentum factor
        momentum: null
        # Dampening for momentum
        dampening: null
        # Enable Nesterov momentum
        nesterov: null

    # ==Learning rate scheduler==
    lr_skd:
        name: plateau
        milestones: null
        # Multiplicative factor of learning rate decay
        gamma: null

        T_max: null
        eta_min: null

        mode: min
        factor: 0.5
        patience: 2

# ==Early Stopping==
patience: 10
mode: 'min'

# ==Evaluator==
evaluator:
    # Evaluation metrics, the choices are as follows:
    #     {'mae', 'rmse', 'rae', 'rrse', 'corr', 'corr_t'}
    eval_metrics: ['rmse', 'mae']
